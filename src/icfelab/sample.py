"""Module for sampling functions from gaussian processes. This includes an executable script for plotting single
sampled functions. This code has partially been generated by a LLM."""
import argparse
import json
import lzma
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import torch
from numpy import ndarray
from scipy.stats import beta
from sklearn.gaussian_process import GaussianProcessRegressor
from tqdm import tqdm

from icfelab.utils import plot_test, create_covariance


def beta_sample_single_value(alpha: float = 5.0, beta_param: float = 5.0, scale: float = 1.0) -> float:
    """
    Samples a length_scale from a Beta distribution and rescales it.

    Parameters:
        alpha (float): Alpha parameter of the Beta distribution.
        beta_param (float): Beta parameter of the Beta distribution.
        scale (float): Maximum value for rescaling (default is 1.0 for [0, 1] range).

    Returns:
        float: A sampled length_scale value.
    """
    sampled_value = beta.rvs(alpha, beta_param)
    return sampled_value * scale


def sample_gp_rbf(x: np.ndarray, gaussian_process: GaussianProcessRegressor) -> ndarray:
    """Samples a function from a Gaussian Process with RBF kernel.
    Parameters:
        x(ndarray): The input values for which to sample the function.
        gaussian_process(GaussianProcessRegressor): The Gaussian Process instance.
    Returns:
        ndarray: sampled function values.
        """
    return gaussian_process.sample_y(x, n_samples=1).ravel()  # type: ignore


def generate_functions(args: argparse.Namespace) -> None:
    """Generate a number of functions and draw random samples of at least length 10 from all function points.
    Add gaussian noise to the sampled data with a random sampled std for each function.
    Args:
        number_functions: number of functions to generate.
        target_path: Path to the target file.
    """
    number_functions, target_path = args.number_functions, Path(args.target_file)
    grid_length = 128
    result_list = []
    sample_size_list = []
    if args.load_sample_sizes is not None:
        with lzma.open(args.load_sample_sizes, "rb") as file:
            sample_sizes = json.loads(file.read().decode("utf-8"))
        assert (
                    len(sample_sizes) >= number_functions), (f"Too few sample size parameters provided. Number of "
                                                             f"functions to generate: {number_functions}, length of "
                                                             f"sample size list: {len(sample_sizes)}")

    for i in tqdm(range(number_functions), desc="Generating functions", unit="functions"):
        rbf_scale = beta_sample_single_value(args.alpha, args.beta)
        co_var, rbf_kernel = create_covariance(rbf_scale, grid_length=grid_length)
        function = np.random.multivariate_normal(mean=np.zeros(co_var.shape[0]), cov=co_var, size=1).squeeze()
        if args.load_sample_sizes:
            data = sample_random_observation_grids(function, args.max, args.min, sample_sizes[i])
        else:
            data = sample_random_observation_grids(function, args.max, args.min)
        sample_size_list.append(len(data["values"]))

        std = abs(np.random.normal(0, 0.1, 1).item())
        data["values"] = add_gaussian_noise(data["values"], 0, std)  # type: ignore

        result_list.append({"target": function.tolist(), "input": data, "rbf_scale": rbf_scale, "std": std})

    if args.plot:
        for i, result in enumerate(result_list):
            plot_test(torch.tensor(result["target"]), torch.tensor(result["input"]["indices"]),
                      torch.tensor(result["input"]["values"]), Path(f"data/generate/{i}"))
    save_compressed_json(result_list, target_path)
    if args.load_sample_sizes is None:
        save_compressed_json(sample_size_list, Path("data/sample_sizes.lzma"))


def sample_random_observation_grids(function: np.ndarray, max: int, min: int, sample_size: Optional[int] = None) -> Dict[str, list]:
    """Generate random grids to select a random number of points from this function."""
    function_size = len(function)
    indices = np.arange(function_size)
    if sample_size is None:
        number_of_points = np.random.randint(min, max + 1)
    else:
        number_of_points = sample_size
    random_grid = np.random.choice(indices, size=number_of_points, replace=False)
    random_grid.sort()
    return {"values": function[random_grid].tolist(), "indices": random_grid.tolist()}


def add_gaussian_noise(data: list, mean: float, std: float) -> ndarray:
    """
    Add gaussian noise to data.
    """
    data_ndarray = np.array(data)
    noise = np.random.normal(loc=mean, scale=std, size=data_ndarray.shape)
    return (data_ndarray + noise).tolist()


def save_compressed_json(serializable_object: Any, target_path: Path) -> None:
    """Compress and save json object."""
    json_str = json.dumps(serializable_object)
    json_bytes = json_str.encode('utf-8')

    target_path.parent.mkdir(parents=True, exist_ok=True)
    with lzma.open(target_path, 'wb') as file:
        file.write(json_bytes)


def get_args() -> argparse.Namespace:
    """
    Defines arguments.

    Returns:
        Namespace with parsed arguments.
    """
    parser = argparse.ArgumentParser(description="Sample random target interpolation functions.")
    parser.add_argument(
        "--target_file",
        "-f",
        type=str,
        default="functions.xz",
        help="File to save sampled functions to."
    )
    parser.add_argument(
        "--number-functions",
        "-n",
        type=int,
        default=100,
        help="Number of functions to generate."
    )
    parser.add_argument(
        "--plot",
        "-p",
        action="store_true",
        help="Plot sampled functions additionally to saving them. This is for debugging purposes, as it takes a lot "
             "of time for large amounts of data."
    )
    parser.add_argument(
        "--load-sample-sizes",
        type=str,
        default=None,
        help="If this contains a path, activates samples size loading. Amound of samples per function will not be random but loaded from "
             "a lzma compressed json file."
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=1,
        help="Beta distribution parameter for gaussian kernel scale."
    )
    parser.add_argument(
        "--beta",
        type=float,
        default=1,
        help="Beta distribution parameter for gaussian kernel scale."
    )
    parser.add_argument(
        "--max",
        type=int,
        default=50,
        help="Maximum number of samples in the observation grid."
    )
    parser.add_argument(
        "--min",
        type=int,
        default=5,
        help="Minimum number of samples in the observation grid"
    )
    return parser.parse_args()


if __name__ == "__main__":
    # todo: load grid length from config
    args = get_args()
    generate_functions(args)
